import os
import re
import requests
import chromadb
from sentence_transformers import SentenceTransformer
import zipfile
from tqdm import tqdm  # æ–°å¢ï¼šé€²åº¦æ¢å¥—ä»¶
import time

# --- é…ç½®å€ ---
BASE_URL = "https://api-gateway.netdb.csie.ncku.edu.tw"
API_KEY = ""
MODEL = "gpt-oss:120b"

# åˆå§‹åŒ–æœ¬åœ° Embedding
print("æ­£åœ¨è¼‰å…¥ Embedding æ¨¡å‹...")
embed_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2') 
print("âœ“ æ¨¡å‹è¼‰å…¥å®Œæˆ")

chroma_client = chromadb.PersistentClient(path="./japan_db")

def prepare_corpus(zip_path, extract_path):
    if not os.path.exists(extract_path):
        print(f"åµæ¸¬åˆ°å£“ç¸®æª”ï¼Œæ­£åœ¨è§£å£“ç¸®è‡³ {extract_path}...")
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            zip_ref.extractall(extract_path)
        print("è§£å£“ç¸®å®Œæˆã€‚")
    else:
        print("è³‡æ–™å¤¾å·²å­˜åœ¨ï¼Œè·³éè§£å£“ã€‚")

def clean_content(text):
    """å°ˆé–€é‡å° Japan-guide çµæ§‹è¨­è¨ˆçš„æ·±åº¦æ¸…æ´—å‡½æ•¸"""
    # 1. ç§»é™¤ç‰¹å®šçš„å´æ¬„æ¨è–¦å€å¡Š
    text = re.sub(r'\[##.*?\]\(/link\.html\?.*?\)', '', text, flags=re.DOTALL)
    
    # 2. ç§»é™¤å°è¦½ã€è´ŠåŠ©å…§å®¹èˆ‡é‡è¤‡æ€§è¨Šæ¯
    noise_patterns = [
        r"Show All .*? Kyoto",
        r"Show All .*? Osaka",
        r"How to get from .*? to .*?",
        r"View itinerary",
        r"Sponsored Story",
        r"Travel News",
        r"Traveling with Kids",
        r"Read our guide",
        r"Read more"
    ]
    for pattern in noise_patterns:
        text = re.sub(pattern, '', text, flags=re.IGNORECASE)

    # 3. ç§»é™¤ HTML æ¨™ç±¤
    text = re.sub(r'<[^>]+>', '', text)

    # 4. ç§»é™¤ Markdown ä¸­çš„ç©ºé€£çµåœ–ç¤ºèˆ‡ç„¡æ•ˆé€£çµ
    text = re.sub(r'\[\]\(.*?\)', '', text)
    text = re.sub(r'\* \[\]', '', text)

    # 5. è™•ç†é€£çºŒç©ºç™½èˆ‡ç©ºè¡Œ
    text = re.sub(r'\n\s*\n', '\n', text)
    text = re.sub(r' {2,}', ' ', text)
    
    return text.strip()

def index_data(md_folder):
    """å»ºç«‹ç´¢å¼• - æ–°å¢è©³ç´°é€²åº¦è¿½è¹¤"""
    CHUNK_SIZE = 300
    OVERLAP = 50
    
    # æª¢æŸ¥æ˜¯å¦éœ€è¦é‡å»ºç´¢å¼•
    try:
        collection = chroma_client.get_collection("docs")
        existing_count = collection.count()
        print(f"ç™¼ç¾å·²å­˜åœ¨çš„ç´¢å¼•ï¼ˆ{existing_count} å€‹ chunksï¼‰")
        user_input = "n"
        if user_input != 'y':
            print("ä¿æŒç¾æœ‰ç´¢å¼•ï¼ŒçµæŸç¨‹åºã€‚")
            return
        chroma_client.delete_collection("docs")
        print("âœ“ å·²æ¸…ç©ºèˆŠç´¢å¼•")
    except:
        print("æœªæ‰¾åˆ°èˆŠç´¢å¼•ï¼Œå°‡å»ºç«‹æ–°ç´¢å¼•")
    
    collection = chroma_client.get_or_create_collection("docs")
    
    # çµ±è¨ˆæ–‡ä»¶æ•¸é‡
    all_files = [f for f in os.listdir(md_folder) if f.endswith(".md")]
    excluded_files = [f for f in all_files if any(x in f.lower() for x in ['privacy', 'terms', 'feedback', 'advertising'])]
    valid_files = [f for f in all_files if f not in excluded_files]
    
    print(f"\nğŸ“Š è³‡æ–™çµ±è¨ˆï¼š")
    print(f"   ç¸½æ–‡ä»¶æ•¸: {len(all_files)}")
    print(f"   æ’é™¤æ–‡ä»¶: {len(excluded_files)}")
    print(f"   å¾…è™•ç†: {len(valid_files)}")
    print(f"\né–‹å§‹å»ºç«‹ç´¢å¼•...\n")
    
    total_chunks = 0
    failed_files = []
    
    # ä½¿ç”¨ tqdm é¡¯ç¤ºé€²åº¦æ¢
    for fn in tqdm(valid_files, desc="è™•ç†æ–‡ä»¶", unit="æª”"):
        try:
            file_path = os.path.join(md_folder, fn)
            
            with open(file_path, 'r', encoding='utf-8') as f:
                raw_text = f.read()

            match = re.search(r'^---\s*(.*?)\s*---\s*(.*)', raw_text, re.DOTALL)
            if not match: 
                failed_files.append((fn, "ç„¡æ³•è§£æ YAML header"))
                continue
            
            header_text, body_text = match.group(1), match.group(2).strip()
            
            # æ¸…æ´—å…§å®¹
            body_text = clean_content(body_text)
            
            # æª¢æŸ¥æ¸…æ´—å¾Œæ˜¯å¦é‚„æœ‰å…§å®¹
            if len(body_text) < 50:
                failed_files.append((fn, "æ¸…æ´—å¾Œå…§å®¹éå°‘"))
                continue
            
            # æå– Metadata
            metadata = {}
            for line in header_text.split('\n'):
                if ':' in line:
                    k, v = line.split(':', 1)
                    metadata[k.strip()] = v.strip()

            clean_metadata = {
                "url": metadata.get("url", ""),
                "title": metadata.get("title", fn.replace('.md', '')),
                "description": metadata.get("description", ""),
                "keywords": metadata.get("keywords", ""),
                "content_type": metadata.get("content_type", "")
            }

            # åˆ‡å‰²ä¸¦å»ºç«‹ç´¢å¼•
            start = 0
            i = 0
            while start < len(body_text):
                end = start + CHUNK_SIZE
                chunk = body_text[start:end]
                
                searchable_text = f"TITLE: {clean_metadata['title']}\nKEYWORDS: {clean_metadata['keywords']}\nINFO: {chunk}"
                
                collection.add(
                    ids=[f"{fn}_{i}"],
                    documents=[searchable_text],
                    embeddings=[embed_model.encode(searchable_text).tolist()],
                    metadatas=[clean_metadata]
                )
                
                total_chunks += 1
                start += (CHUNK_SIZE - OVERLAP)
                i += 1
                if len(body_text) - start < 50: break
        
        except Exception as e:
            failed_files.append((fn, str(e)))
    
    # é¡¯ç¤ºçµæœ
    print(f"\n{'='*60}")
    print(f"âœ“ ç´¢å¼•å»ºç«‹å®Œæˆï¼")
    print(f"  - æˆåŠŸè™•ç†: {len(valid_files) - len(failed_files)} å€‹æ–‡ä»¶")
    print(f"  - ç¸½ Chunks: {total_chunks}")
    print(f"  - è³‡æ–™åº«å¤§å°: {collection.count()} å€‹å‘é‡")
    
    if failed_files:
        print(f"\nå¤±æ•—æ–‡ä»¶ ({len(failed_files)}):")
        for fn, reason in failed_files[:5]:  # åªé¡¯ç¤ºå‰5å€‹
            print(f"   - {fn}: {reason}")
        if len(failed_files) > 5:
            print(f"   ... é‚„æœ‰ {len(failed_files)-5} å€‹")
    print(f"{'='*60}\n")

def query_rag_with_filter(location, question):
    """æ”¹è‰¯ç‰ˆæª¢ç´¢å‡½æ•¸"""
    collection = chroma_client.get_collection("docs")
    
    query_text = f"{location} {question}"
    query_emb = embed_model.encode(query_text).tolist()
    
    print(f"ğŸ” æ­£åœ¨æª¢ç´¢é—œæ–¼ã€Œ{location}ã€çš„è³‡è¨Š...")
    
    results = collection.query(
        query_embeddings=[query_emb],
        n_results=5, 
        where_document={"$contains": location} 
    )

    if not results['documents'][0]:
        return f"âŒ æ‰¾ä¸åˆ°é—œæ–¼ã€Œ{location}ã€çš„å…·é«”è³‡è¨Šã€‚"

    context_list = []
    for doc, meta in zip(results['documents'][0], results['metadatas'][0]):
        context_list.append(f"ã€è³‡æ–™ä¾†æºï¼š{meta['title']}ã€‘\nURL: {meta['url']}\nå…§å®¹: {doc}")
    
    return "\n---\n".join(context_list)

if __name__ == "__main__":
    print("=" * 60)
    print("æ—¥æœ¬æ—…éŠ RAG ç³»çµ± - ç´¢å¼•å»ºç«‹å·¥å…·")
    print("=" * 60 + "\n")
    
    zip_file = r"C:\è¨ˆç®—ç†è«–\travel_agent\japan_deep_corpus.zip"
    data_path = r"C:\è¨ˆç®—ç†è«–\travel_agent\japan_deep_corpus"
    
    # æ­¥é©Ÿ 1: æº–å‚™è³‡æ–™
    if os.path.exists(zip_file):
        prepare_corpus(zip_file, data_path)
    
    # æ­¥é©Ÿ 2: å»ºç«‹ç´¢å¼•
    start_time = time.time()
    index_data(data_path)
    elapsed = time.time() - start_time
    print(f"â±ï¸  ç¸½è€—æ™‚: {elapsed:.2f} ç§’")
    
    # æ­¥é©Ÿ 3: æ¸¬è©¦æª¢ç´¢
    print("\n" + "="*60)
    print("æ¸¬è©¦æª¢ç´¢åŠŸèƒ½")
    print("="*60)
    loc = "Osaka"
    q = "å¤§é˜ªæœ‰å“ªäº›é ç®—ä½çš„æ™¯é»æˆ–çœéŒ¢äº¤é€šå·¥å…·ï¼Ÿ"
    result = query_rag_with_filter(loc, q)
    print(len(result))
